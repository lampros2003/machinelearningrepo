{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Μάθημα: Μηχανική Μάθηση (Μέθοδοι Επεξεργασίας με Τεχνικές Μηχανικής Μάθησης)\n",
    "\n",
    "**Διδάσκων:** Γεώργιος Μουστακίδης\n",
    "\n",
    "**Φοιτητής:**\n",
    "\n",
    "- **Α.Μ.:** 1092732\n",
    "- **Ονοματεπώνυμο:** Λάμπρος Αβούρης\n",
    "\n",
    "**Άσκηση:** Άσκηση 1\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: α)\n",
    "\n",
    "### Απάντηση\n",
    "Από την εκφώνηση έχουμε τα σημαντικά δεδομένα:\n",
    "\n",
    "- **Α)** Τα $( x_1, x_2 )$ είναι ανεξάρτητα, άρα $f(x_1, x_2) = f(x_1) f(x_2) $.\n",
    "- **Β)** Οι δύο υποθέσεις έχουν ίσα priors, $ P(H_0) = P(H_1) = 0.5 $.\n",
    "\n",
    "Έτσι, δημιουργείται ο παρακάτω πίνακας σύμφωνα με τον Bayes:\n",
    "\n",
    "|                | $H_0 $ | $H_1 $ |\n",
    "|----------------|-----------|-----------|\n",
    "| Prior Probability | 0.5       | 0.5       |\n",
    "| P. D.F.           | $ f_0(x) $ | $ f_1(x) $ |\n",
    "| Correct Decision Cost | $ C_{00} $ | $ C_{11} $ |\n",
    "| Incorrect Decision Cost | $ C_{01} $ | $ C_{10} $ |\n",
    "\n",
    "### Optimum Bayes Test\n",
    "Σύμφωνα με τη θεωρία (Lecture 5, pg 8), το βέλτιστο τεστ Bayes είναι το παρακάτω likelihood ratio test:\n",
    "\n",
    "$$\n",
    "H_1 \\text{, if } \\frac{f_1(x_1, x_2)}{f_0(x_1, x_2)} > \\frac{P(H_0) C_{00} - P(H_0) C_{01}}{P(H_1) C_{10} - P(H_1) C_{11}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_0 \\text{, if } \\frac{f_1(x_1, x_2)}{f_0(x_1, x_2)} < \\frac{P(H_0) C_{00} - P(H_0) C_{01}}{P(H_1) C_{10} - P(H_1) C_{11}}\n",
    "$$\n",
    "\n",
    "Λόγω των (Α) και (Β), το τεστ γίνεται:\n",
    "\n",
    "$$\n",
    "H_1 \\text{, if } \\frac{f_1(x_1) \\cdot f_1(x_2)}{f_0(x_1) \\cdot f_0(x_2)} > \\frac{C_{00} - C_{01}}{C_{10} - C_{11}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_0 \\text{, if } \\frac{f_1(x_1) \\cdot f_1(x_2)}{f_0(x_1) \\cdot f_0(x_2)} < \\frac{C_{00} - C_{01}}{C_{10} - C_{11}}\n",
    "$$\n",
    "\n",
    "### Ισότητα Μέσου Κόστους και Πιθανότητας Σφάλματος\n",
    "Γνωρίζουμε ότι το average cost ισούται με την error probability όταν:\n",
    "\n",
    "$$\n",
    "\\frac{C_{00} - C_{01}}{C_{10} - C_{11}} = 1\n",
    "$$\n",
    "(Lecture 5, pg 8)\n",
    "\n",
    "Σε αυτήν την περίπτωση, ο πίνακας έχει τη μορφή:\n",
    "\n",
    "|                | $H_0 $ | $H_1 $ |\n",
    "|----------------|-----------|-----------|\n",
    "| Prior Probability | 0.5       | 0.5       |\n",
    "| P. D.F.           | $ f_0(x) $ | $ f_1(x) $ |\n",
    "| Correct Decision Cost | $0 $ | $ 0 $ |\n",
    "| Incorrect Decision Cost | $ 1 $ | $ 1 $ |\n",
    "\n",
    "Δηλαδή έχω ίσο μοναδιαίο κόστος για κάθε λάθος και 0 κόστος για σωστή απάντηση\n",
    "\n",
    "Η τελική μορφή του τεστ λοιπόν είναι:\n",
    "\n",
    "$$\n",
    "H_1 \\text{, if } \\frac{f_1(x_1) \\cdot f_1(x_2)}{f_0(x_1) \\cdot f_0(x_2)} > 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_0 \\text{, if } \\frac{f_1(x_1) \\cdot f_1(x_2)}{f_0(x_1) \\cdot f_0(x_2)} < 1\n",
    "$$\n",
    "\n",
    "### Εκφράσεις για τις Κατανομές\n",
    "- $ H_0: f_0(x) \\sim N(0, 1) $, άρα:\n",
    "\n",
    "$$\n",
    "f_0(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\n",
    "$$\n",
    "\n",
    "- $H_1: f_1(x) \\sim 0.5[N(-1, 1) + N(1, 1)] $, άρα:\n",
    "\n",
    "$$\n",
    "f_1(x) = \\frac{1}{2} \\frac{1}{\\sqrt{2\\pi}} \\left(e^{-\\frac{(x+1)^2}{2}} + e^{-\\frac{(x-1)^2}{2}}\\right)\n",
    "$$\n",
    "\n",
    "### Λόγος Πιθανοφάνειας\n",
    "$$\n",
    "\\frac{f_1(x)}{f_0(x)} = \\frac{\\frac{1}{2} \\left(e^{-\\frac{(x+1)^2}{2}} + e^{-\\frac{(x-1)^2}{2}}\\right)}{e^{-\\frac{x^2}{2}}}\n",
    "$$\n",
    "\n",
    "Με την ανεξαρτησία των $ x_1, x_2 $ (Α):\n",
    "\n",
    "$$\n",
    "\\frac{f_1(x_1, x_2)}{f_0(x_1, x_2)} = \\frac{f_1(x_1)}{f_0(x_1)} \\cdot \\frac{f_1(x_2)}{f_0(x_2)}\n",
    "$$\n",
    "\n",
    "Η τελική σχέση του τεστ είναι:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\left(e^{-\\frac{(x_1 + 1)^2}{2}} + e^{-\\frac{(x_1 - 1)^2}{2}}\\right) \\cdot \\frac{1}{2} \\left(e^{-\\frac{(x_2 + 1)^2}{2}} + e^{-\\frac{(x_2 - 1)^2}{2}}\\right) > e^{-\\frac{x_1^2}{2}} e^{-\\frac{x_2^2}{2}}\n",
    "$$\n",
    "\n",
    "### Απλοποίηση\n",
    "Απλοποιώντας:\n",
    "\n",
    "$$\n",
    "e^{-\\frac{(x_i \\pm 1)^2}{2}} = e^{-\\frac{x_i^2}{2}} \\cdot e^{\\mp x_i} \\cdot e^{-\\frac{1}{2}}\n",
    "$$\n",
    "\n",
    "Άρα:\n",
    "\n",
    "$$\n",
    "\\frac{1}{4} \\left(e^{-\\frac{x_1^2}{2}}e^{-\\frac{1}{2}}(e^{x_1} + e^{-x_1})\\right) \\left(e^{-\\frac{x_2^2}{2}}e^{-\\frac{1}{2}}(e^{x_2} + e^{-x_2})\\right) > e^{-\\frac{x_1^2}{2}} e^{-\\frac{x_2^2}{2}}\n",
    "$$\n",
    "\n",
    "Γνωρίζοντας ότι:\n",
    "\n",
    "$$\n",
    "e^{x} + e^{-x} = 2 \\cosh(x)\n",
    "$$\n",
    "\n",
    "Η τελική μορφή είναι:\n",
    "\n",
    "$$\n",
    "H_1 \\text{, if } \\cosh(x_1) \\cosh(x_2) > e\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_0 \\text{, if } \\cosh(x_1) \\cosh(x_2) < e\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: b)\n",
    "\n",
    "### Απάντηση\n",
    "Θέλω να υλοποιήσω, σε python πρόγραμμα μια προσομοίωση της παραπάνω διαδικασίας για τον υπολογισμό error probability.\n",
    "Αρχικά κάνω import  τη βιβλιοθήκη numpy και την συνάρτηση norm από την schipy.stats.\n",
    "Με την numpy θα μπορώ να κάνω περίπλοκους αριθμητικούς υπολογισμούς γρήγορα, και με την norm θα μπορώ να δημιουργήσω samples από normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ορίζω τις συναρτήσεις πυκνότητας των δύο συναρτήσεων όπως τις υπολόγισα παραπάνω.\\\n",
    "Επίσης δημιουργώ την συνάρτηση που προσομοιώνει το Bayes test με δύο τρόπους, by default με την απλοποιημένη σχέση\n",
    "με τα υπερβολικά συνημίτονα η αλλιώς άμεσα με τις συναρτήσεις. \\\n",
    "Σημαντικό ότι λογαρίθμιζω τα ratios για να μπορώ να κάνω πιο ευσταθείς και ακριβείς υπολογισμούς."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f0(x):\n",
    "    \"\"\"Probability density function for H0\"\"\"\n",
    "    return 1 / np.sqrt(2 * np.pi) * np.exp(-x**2 / 2)\n",
    "\n",
    "def f1(x):\n",
    "    \"\"\"Probability density function for H1\"\"\"\n",
    "    return 0.5 * (norm.pdf(x, -1, 1) + norm.pdf(x, 1, 1))\n",
    "\n",
    "def bayes_test(x1, x2,simplified =\"simple\"):\n",
    "    \"\"\"Optimal Bayes test\"\"\"\n",
    "    if simplified == \"simple\":\n",
    "        log_likelihood_ratio = np.log(np.cosh(x1)*np.cosh(x2))\n",
    "        if log_likelihood_ratio > np.log(np.e):\n",
    "            return 1  # Decide H1\n",
    "        else:\n",
    "            return 0  # Decide H0\n",
    "    \n",
    "    log_likelihood_ratio = np.log(f1(x1) / f0(x1)) + np.log(f1(x2) / f0(x2))\n",
    "    if log_likelihood_ratio > 0:\n",
    "        return 1  # Decide H1\n",
    "    else:\n",
    "        return 0  # Decide H0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δημιουργώ $10^6$ ζεύγη samples για κάθε υπόθεση.\\\n",
    "Εφαρμόζω το bayes test στην απλοποιημένη του μορφή σε κάθε ένα απο αυτά τα ζεύγη samples, αποθηκεύοντας τα αποτελέσματα\n",
    "στις λίστες y0 για $H_0$ y1 για $H_1$.\\\n",
    "Μετρώ μετά τα λάθη που έχει κάνει το τεστ και τέλος βρίσκω το μέσω σφάλμα.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10**6\n",
    "\n",
    "# Generate samples from H0 \n",
    "x0_1, x0_2 = np.random.normal(0, 1, (2, num_samples))\n",
    "# Generate samples from H1\n",
    "\n",
    "x1_1 = np.random.choice([-1, 1], num_samples, p=[0.5, 0.5]) + np.random.normal(0, 1, num_samples)\n",
    "x1_2 = np.random.choice([-1, 1], num_samples, p=[0.5, 0.5]) + np.random.normal(0, 1, num_samples)\n",
    "\n",
    "# Apply the Bayes test\n",
    "y0 = [bayes_test(x0_1[i], x0_2[i]) for i in range(num_samples)]\n",
    "y1 = [bayes_test(x1_1[i], x1_2[i]) for i in range(num_samples)]\n",
    "\n",
    "# Calculate error probabilities\n",
    "error_p0 = np.sum(y0) / num_samples\n",
    "error_p1 = 1-np.sum(y1)/ num_samples\n",
    "total_error = 0.5 * error_p0 + 0.5 * error_p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τυπώνω τα αποτελέσματα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error probability for H0: 0.2826\n",
      "Error probability for H1: 0.4232\n",
      "Total error probability: 0.3529\n"
     ]
    }
   ],
   "source": [
    "print(f\"Error probability for H0: {error_p0:.4f}\")\n",
    "print(f\"Error probability for H1: {error_p1:.4f}\")\n",
    "print(f\"Total error probability: {total_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: b)\n",
    "\n",
    "### Απάντηση\n",
    "#### Αρχικά :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m W1, b1, W2, b2\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Generate training data\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Train the neural network\u001b[39;00m\n\u001b[0;32m     95\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m train_nn(X_train, y_train, input_size, hidden_size, output_size, learning_rate, epochs)\n",
      "Cell \u001b[1;32mIn[33], line 32\u001b[0m, in \u001b[0;36mgenerate_data\u001b[1;34m(num_samples)\u001b[0m\n\u001b[0;32m     29\u001b[0m x1_2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], num_samples, p\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, num_samples)\n\u001b[0;32m     30\u001b[0m h1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((x1_1, x1_2))\n\u001b[1;32m---> 32\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(num_samples)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnum_samples):\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 2        # x1 and x2\n",
    "hidden_size = 20      # Hidden layer size\n",
    "output_size = 1       # Output layer size\n",
    "learning_rate = 0.01  # Learning rate\n",
    "epochs = 1000         # Number of training epochs\n",
    "\n",
    "# Activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-15  # To avoid log(0)\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Generating the training data for the condition cosh(x1) * cosh(x2) > e\n",
    "def generate_data(num_samples=200):\n",
    "    h0 = np.random.normal(0, 1, (2, num_samples))\n",
    "    # Generate samples from H1\n",
    "\n",
    "    x1_1 = np.random.choice([-1, 1], num_samples, p=[0.5, 0.5]) + np.random.normal(0, 1, num_samples)\n",
    "    x1_2 = np.random.choice([-1, 1], num_samples, p=[0.5, 0.5]) + np.random.normal(0, 1, num_samples)\n",
    "    h1 = np.concatenate((x1_1, x1_2))\n",
    "\n",
    "    X = np.concatenate((h0, h1), axis=0)\n",
    "    y = np.zeros(num_samples)\n",
    "    for i in range(2*num_samples):\n",
    "        x1, x2 = X[i]\n",
    "        if np.cosh(x1) * np.cosh(x2) > np.e:\n",
    "            y[i] = 1\n",
    "    return X, y\n",
    "\n",
    "# Neural Network initialization\n",
    "def initialize_weights(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(input_size, hidden_size) * 0.01  # Weights for input to hidden\n",
    "    b1 = np.zeros((1, hidden_size))  # Biases for hidden layer\n",
    "    W2 = np.random.randn(hidden_size, output_size) * 0.01  # Weights for hidden to output\n",
    "    b2 = np.zeros((1, output_size))  # Biases for output layer\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Forward pass\n",
    "def forward(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)  # Hidden layer activation\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)  # Output layer activation\n",
    "    return A1, A2\n",
    "\n",
    "# Training the neural network using cross-entropy method\n",
    "def train_nn(X_train, y_train, input_size, hidden_size, output_size, learning_rate, epochs):\n",
    "    # Initialize weights\n",
    "    W1, b1, W2, b2 = initialize_weights(input_size, hidden_size, output_size)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        A1, A2 = forward(X_train, W1, b1, W2, b2)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = cross_entropy_loss(y_train, A2)\n",
    "        \n",
    "        # Backpropagation\n",
    "        dA2 = A2 - y_train.reshape(-1, 1)  # Reshaping y_train to match output shape\n",
    "        dW2 = np.dot(A1.T, dA2)  # Gradient for weights between hidden and output layer\n",
    "        db2 = np.sum(dA2, axis=0, keepdims=True)  # Gradient for bias of output layer\n",
    "\n",
    "        dA1 = np.dot(dA2, W2.T) * sigmoid_derivative(A1)  # Gradient for hidden layer\n",
    "        dW1 = np.dot(X_train.T, dA1)  # Gradient for weights between input and hidden layer\n",
    "        db1 = np.sum(dA1, axis=0, keepdims=True)  # Gradient for bias of hidden layer\n",
    "\n",
    "        # Update weights\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Generate training data\n",
    "X_train, y_train = generate_data()\n",
    "\n",
    "# Train the neural network\n",
    "W1, b1, W2, b2 = train_nn(X_train, y_train, input_size, hidden_size, output_size, learning_rate, epochs)\n",
    "\n",
    "# Testing the neural network\n",
    "def test_nn(X_test, W1, b1, W2, b2):\n",
    "    _, A2 = forward(X_test, W1, b1, W2, b2)\n",
    "    return A2\n",
    "\n",
    "# Example test\n",
    "X_test = np.array([[1.0, 2.0], [0.5, -0.5], [-1.5, 2.0], [-2.0, -2.0]])\n",
    "predictions = test_nn(X_test, W1, b1, W2, b2)\n",
    "print(\"Predictions (probabilities):\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
